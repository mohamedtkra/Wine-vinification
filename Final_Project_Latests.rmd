---
title: "Final Project"
author: "Charaf Lachouri, Mohamed Tounkara, Rohan Thaliachery, Tatiana Uklist"
date: "2022-11-30"
output:
  word_document: default
  pdf_document: default
editor_options:
  markdown:
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

***Introduction***

Wine making, also known as vinification, is the process of producing
wine, starting from the selection of fruit (typically grapes), its
fermentation, and the bottling of the finished product. This art form of
a process stretches over millennium with the first documented instances
being around since between 5000 - 5400 BC. The process has been
perfected and celebrated over the years while at the same time, the
finished product itself has been beloved and held both religiously and
socially sacred ever since. And as the process has grown, the
classification of wine has evolved as well.

Today, the 100 point scale is what is widely used with top rated wines
usually rating above 90. The finalized ranking is typically the average
of all the points given to that wine. The tasters are usually looking at
taste and physical features such as color, sugar-level, growing method,
and climate that the fruits are grown in. The tastings are usually done
blindly in order to prevent any bias towards brands, vineyards or
winemakers. And while the taste features are important, the taster's own
personal preference will always bias their ranking. That got our group
thinking of alternative ways that the wine could in theory be ranked. We
were curious about what the chemical composition did to the wine
classification.

Using multinomial regression analysis, we will create a model to predict
the wine class. We will train our multinomial regression analysis model
over a hundred iterations. After training the data, we will split the
data into thirty percent for training and seventy percent for testing
the data in order to predict the wine classes.

# Data set information

Source:

Original Owners:

Forina, M. et al, PARVUS - An Extendible Package for Data Exploration,
Classification and Correlation. Institute of Pharmaceutical and Food
Analysis and Technologies, Via Brigata Salerno, 16147 Genoa, Italy.

Donor:

Stefan Aeberhard, email: stefan '\@' coral.cs.jcu.edu.au

These data are the results of a chemical analysis of wines grown in the
same region in Italy but derived from three different cultivars. The
analysis determined the quantities of 13 constituents found in each of
the three types of wines.

I think that the initial data set had around 30 variables, but for some
reason I only have the 13 dimensional version. I had a list of what the
30 or so variables were, but a.) I lost it, and b.), I would not know
which 13 variables are included in the set.

The attributes are (dontated by Riccardo Leardi, riclea '\@'
anchem.unige.it) 1) Alcohol 2) Malic acid 3) Ash 4) Alcalinity of ash 5)
Magnesium 6) Total phenols 7) Flavanoids 8) Nonflavanoid phenols 9)
Proanthocyanins 10) Color intensity 11) Hue 12) OD280/OD315 of diluted
wines 13) Proline

# 1. Loading Packages and Libraries

```{r}
library(tidyverse) # To structure, manipulate and visualize data.
library(car) # To test, transform and visualize data.
library(MASS) # To do data transformation.
library(ggplot2) # To do data visualization.
library(KODAMA) # To do unsupervised features prediction.
library(dplyr) # To do data manipulation
library(nnet) # To do neural network classification 
```

# 2. Loading the data and Eploratory Analysis

```{r}
wine <- read.csv("Wine_Dataset.csv")
attach(wine)
head(wine, 10)
```

The dataset contains information about 178 unique wines divided into
three categories which are represented by 1 to 3 numbers. The dependent
variable here is Classes.

```{r}
# Data Dimensions
dim(wine)
```

# 3. Statistical Summary

In our dataset, the average alcohol percentage is 13%

```{r}
# Descriptions 
summary(wine)
```

# We have identify identify 3 classes which will be used to classify the wine based on several variables

```{r}
#Counts of classes in data
table(Classes)
```

#Our dataset is structured around 2 types of data: 3 Integers (Classes,
Magnesium and Proline) and 11 Numeric data

```{r}
# Checking the structure of wine dataset
str(wine)
```

# 4. Data cleaning (remove noise and inconsistent data)

Using sum and is.na function we will check for any missing values in our
dataset. If we find any missing values, we will remove it from our
dataset by na.omit() function and check the dimension for data set.

```{r}
# Missing values ? 
sum(is.na(wine))
```

No missing values found. \# Changing our response variable to a factor
Changing our variables in factors helped us to identify the different
types of classes. In our case classes are between 1 and 3

```{r}
Classes <- as.factor(Classes)
Classes
```

```{r}
# Checking for correlation between the predictors 
cor(wine[,-1])
heatmap(abs(cor(wine[,-1])))
```

We have slightly correlated predictors : 1. "Alcohol" and "Proline"
(0.64). 2. "Hue" and "Malic.acid" (-0.56), 3.
"OD280.OD315.of.diluted.wines" and "Flavanoids" (0.79). 4.
"OD280.OD315.of.diluted.wines" and "Total.phenols" (0.70).

Let's check the significance of each predictor !

```{r}
# Multiple Linear Regression
fit = lm(Classes ~ Alcohol+Malic.acid+Ash+Alcalinity.of.ash+Magnesium+Total.phenols+Flavanoids+Nonflavanoid.phenols+Proanthocyanins+Color.intensity+Hue+OD280.OD315.of.diluted.wines+Proline, data=wine)
summary(fit)
```

In this case "TRUE" means that the P value\< 0.05 as a result it will
show that there is significant relationship between the intercept and
the those variables.

```{r}
# P-value of each coefficient less than 0.05
summary(fit)$coef[,4] < 0.05
```

```{r}
# Variance Inflation Factor (VIF) 
round(vif(fit),2)
```

From the heat map correlated predictors and the non-significant
coefficients. We decided to remove the following independent variables :
"Hue", "Magnesium", "Proanthocyanins" and "Ash".

## 5. Splitting the data into train and test

To begin, we'll create a fake indicator to indicate whether a row is in
the training or testing data set. In an ideal world, we'd have 70%
training data and 30% testing data, which would provide the highest
level of accuracy.

```{r}
# Using sample_frac to create 30 - 70 slipt into test and train
train <- sample_frac(wine, 0.3)
sample_id <- as.numeric(rownames(train)) # rownames() returns character so as.numeric
test <- wine[-sample_id,] 
head(test)
```

We use mutinom() function from {nnet} package and relevel() function to
set up the Classes baseline level. Multinomial regression is an
extension of binomial logistic regression allows us to predict a
categorical dependent variable which has more than two levels.

```{r}
# Setting up the baseline 
train$Classes <- relevel(factor(train$Classes), ref = "3")
train$Classes
```

# 6. Training the multinomial model

```{r}
  multinom.fit <- multinom (Classes ~ Alcohol+Malic.acid+Alcalinity.of.ash+Total.phenols+Flavanoids+Nonflavanoid.phenols+Color.intensity+OD280.OD315.of.diluted.wines+Proline, data = train)
 
# Checking the model
summary(multinom.fit)
```

The output of summary contains the table for coefficients and a table
for standard error. Each row in the coefficient table corresponds to the
model equation. The first row represents the coefficients for Class 2
wine in comparison to our baseline which is Class 3 wine and the second
row represents the coefficients for Class 2 wine in comparison to our
baseline which is Class 3 wine.

The output coefficients are represented in the log of odds.

This ratio of the probability of choosing Class 2 wine over the baseline
that is Class 3 wine is referred to as relative risk (often described as
odds). However, the output of the model is the log of odds. To get the
relative risk IE odds ratio, we need to exponentiate the coefficients.

```{r}
## extracting coefficients from the model and exponentiate
exp(coef(multinom.fit))
```

Here a value of 1 represents that there is no change. However, a value
greater than 1 represents an increase and value less than 1 represents a
decrease.

```{r}
head(probability.table <- fitted(multinom.fit))
```

The table above indicates that the probability of the 1st obs being
Class 2 is 100 %, being Class 1 is 0 % and being Class 3 is 0 % and so
on with other obs.

We will now check the model accuracy by building classification table.
So let us first build the classification table for training data set and
calculate the model accuracy.

# 7. The Prediction

```{r}
# Predicting the values for train dataset
train$precticed <- predict(multinom.fit, newdata = train, "class")
 
# Building classification table
ctable <- table(train$Classes, train$precticed)
ctable
```

```{r}
# Calculating accuracy - sum of diagonal elements divided by total obs
round((sum(diag(ctable))/sum(ctable))*100,2)
```

Accuracy in training dataset is 100% which is perfect. We now repeat the
above on the unseen dataset that tests dataset.

```{r}
# Predicting the values for test dataset
test$precticed <- predict(multinom.fit, newdata = test, "class")

# Building classification table
ctable <- table(test$Classes, test$precticed)
ctable
```

Our model perfectly classified class 1 data points, misclassified 6 out
of 71 data points on class 2 and misclassified 2 out of 48 data points
on class 3.

```{r}
accuracy <- round(mean(test$Classes == test$precticed)*100, 2)
accuracy
```

Our Multinomial Logistic Regreesion model prediction accuracy is 90.4 %
which is very good.

# 8. Improving the prediction accuracy

Let's see if we can improve the prediction accuracy of our model by
transforming the predictor variables.

```{r}
# Plotting the pairs plot of the data
pairs(Classes ~ Alcohol+Malic.acid+Alcalinity.of.ash+Total.phenols+Flavanoids+Nonflavanoid.phenols+Color.intensity+OD280.OD315.of.diluted.wines+Proline)
```

Using powerTransform() to do a BoxCox on the predictor variables.

```{r}
summary(powerTransform(cbind(Alcohol,Malic.acid,Alcalinity.of.ash,Total.phenols,Flavanoids,Nonflavanoid.phenols,Color.intensity,OD280.OD315.of.diluted.wines,Proline)))
```

Most of the data is scruntched towards 0, So, Let's Log transform all
the predictors.

Now, an inverseResponsePlot:

```{r}
multinom.fit_trns <- multinom (Classes ~ log(Alcohol)+log(Malic.acid)+log(Alcalinity.of.ash)+log(Total.phenols)+log(Flavanoids)+log(Nonflavanoid.phenols)+log(Color.intensity)+log(OD280.OD315.of.diluted.wines)+log(Proline), data = train)
```

```{r}
pairs(Classes ~ log(Alcohol)+log(Malic.acid)+log(Alcalinity.of.ash)+log(Total.phenols)+log(Flavanoids)+log(Nonflavanoid.phenols)+log(Color.intensity)+log(OD280.OD315.of.diluted.wines)+log(Proline))
```

well, we can see that we've gotten a slight improvement on couple
predictors.

```{r}
summary(multinom.fit_trns)
```

# 9. The Prediction of the new model

```{r}
# Predicting the values for train dataset
train$precticed <- predict(multinom.fit_trns, newdata = train, "class")
 
# Building classification table
ctable <- table(train$Classes, train$precticed)
ctable
```

100% Training Prediction rate. Perfect !

```{r}
# Predicting the values for test dataset
test$precticed <- predict(multinom.fit_trns, newdata = test, "class")

# Building classification table
ctable <- table(test$Classes, test$precticed)
ctable
```

```{r}
accuracy <- round(mean(test$Classes == test$precticed)*100,)
accuracy
```

*The log transformation of the predictor variables did a good job on
improving the prediction accuracy of our model, bringing it up from
90.3% to 97.6% which is an excellent accuracy rate.*

------------------------------------------------------------------------

# **Conclusion**

The purpose of the project was develop a multinomial regression analysis
model that would use the alcohol level, malic acid, alkalinity of ash,
the total phenol's, the flavanoids, the nonflavoid phenols, the color
intensity, the OD280 OD315 of diluted wine, hue and proline to predict
the class of wine.

Before removing "Hue", "Magnesium", "Proanthocyanins" and "Ash", we
found that our model was consistently misclassifying class two and three
, which was surprising because we thought that the classification would
be more evenly misclassified.

Once we selected our final predictors, we found that the model was able
to predict the class of wine with a consistant accuracy between 80-90%,
only missclassifying class three which is an improvement. Out of
curiosity, we transformed the multinomial regression analysis model
which ended up improving our accuracy to over 95%.

# Limitations

Like all models, our model was not perfect and definitely had its
limitations. The data was very limited so we were not able to show the
accuracy between wines produced in different regions and if that had an
impact. In the future, we would use more data to train and compare. And
potentially add or replace different variables.
